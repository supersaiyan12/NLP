{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Line to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to industry estimates, only 21% of the available data is present in structured form. Data is being generated as we speak, as we tweet, as we send messages on Whatsapp and in various other activities. Majority of this data exists in the textual form, which is highly unstructured in nature.\n",
    "\n",
    "\n",
    "- Few notorious examples include – tweets / posts on social media, user to user chat conversations, **news, blogs and articles**, product or **services reviews** and patient records in the **Healthcare sector**.\n",
    "\n",
    "\n",
    "- Despite having high dimension data, the information present in it is not directly accessible unless it is processed (read and understood) manually or analyzed by an automated system.\n",
    "\n",
    "\n",
    "- In order to produce significant and actionable insights from text data, it is important to get acquainted with the techniques and principles of Natural Language Processing (NLP).\n",
    "\n",
    "## List of things can NLP Do :-)\n",
    "\n",
    "#### 1. Introduction to NLP\n",
    "\n",
    "#### 2. Text Preprocessing\n",
    "\n",
    "- [**Noise Removal**](#noise_removal): A general approach for noise removal is to prepare a dictionary of noisy entities to eliminate them from text\n",
    "- [**Lexicon Normalization**](#lexicon_normalization): Another type of textual noise is about the multiple representations exhibited by single word\n",
    "    - [Lemmatization](#lemmatization): Lemmatization is an organized & step by step procedure of obtaining the root form of the word\n",
    "    - [Stemming](#stemming): Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) - From a word\n",
    "- [**Object Standardization**](#object_standardization) : Replace social media slangs from a text\n",
    "\n",
    "#### 2.1 **from Speech**\n",
    "- [**Speech to text**](#object_standardization) : Convert Speech or voice data into a text format\n",
    "\n",
    "\n",
    "   \n",
    "#### 3. Text to Features (Feature Engineering on text data)\n",
    "- [**Syntactical Parsing**](#Syntactical Parsing): This involves the analysis of words for grammar and their arrangement in a manner that shows the relationships among the words\n",
    "    - [Dependency Grammar](#Dependency Grammar): Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words)\n",
    "    - [Part of Speech Tagging](#Part of Speech Tagging): Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag\n",
    "- [**Entity Parsing**] (#Entity Parsing): Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing\n",
    "    - [Named Entity Recognition](#Named Entity Recognition): The process of detecting the named entities such as person names, location names, company names etc\n",
    "    - [Topic Modelling](#Topic Modelling): a process of automatically identifying the topics present in a text corpus\n",
    "    - [N-Grams](#N-Grams): A combination of N words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words (Unigrams) as features.\n",
    "- [**Statistical features**] (#Statistical features): \n",
    "    - [TF – IDF](#TF – IDF): TF-IDF is a weighted model. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering.\n",
    "    - [Frequency / Density Features](#Frequency / Density Features): Some of the features are: _Word Count, Sentence Count, Punctuation Counts and Industry specific word counts_.\n",
    "    - [Readability Features](#Readability Features): Other types of measures include __readability measures__ such as _syllable counts, smog index and flesch reading ease_\n",
    "\n",
    "    \n",
    "#### 4. Important tasks of NLP\n",
    "- [**Text Classification**](#Text Classification): Email Spam Identification, topic classification of news, sentiment classification and organization of web pages by search engines\n",
    "- [**Text Matching**](#Text Matching / Similarity): One of the important areas of NLP is the matching of text objects to find similarities.\n",
    "    - [Levenshtein Distance](#Levenshtein Distance): the minimum number of edits needed to transform one string into the other\n",
    "    - [Phonetic Matching](#Phonetic Matching): takes a keyword as input (person’s name, location name etc) and produces a character string that identifies a set of words that are (roughly) phonetically similar\n",
    "    - [Flexible String Matching](#Flexible String Matching): A complete text matching system includes different algorithms pipelined together to compute variety of text variations\n",
    "    - [Cosine Similarity](#Cosine Similarity): When the text is represented as vector notation, a general cosine similarity can also be applied in order to measure vectorized similarity\n",
    "- [**Coreference Resolution**](#Coreference Resolution): a process of finding relational links among the words (or phrases) within the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to Natural Language Processing\n",
    "\n",
    "NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n",
    "\n",
    "Few terms that are used in the article:\n",
    "\n",
    "   - Tokenization – process of converting a text into tokens\n",
    "   - Tokens – words or entities present in the text\n",
    "   - Text object – a sentence or a phrase or a word or an article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "Since, text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n",
    "\n",
    "It is predominantly comprised of three steps:\n",
    "\n",
    "   - **Noise Removal**          - ('a','an','I','to',etc)\n",
    "   - **Lexicon Normalization**  - (Suffix or Prefix that are addition to the root words)\n",
    "   - **Object Standardization** - (Words that aren't even exist in dictionary but that are there in data - e.g(awsm-awesome)\n",
    "   \n",
    "The following image shows the architecture of text preprocessing pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='noise_removal'></a>\n",
    "## 2.1 Noise Removal\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
    "\n",
    "For example – language stopwords (commonly used words of a language – is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n",
    "\n",
    "A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary.\n",
    "\n",
    "Following is the python code for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove noisy words from a text\n",
    "noise_list = ['is','a','this','...']\n",
    "\n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split()\n",
    "    noise_free_words = [word for word in words if word not in noise_list]\n",
    "    noise_free_text = ' '.join(noise_free_words)\n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"... this is a sample text ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from this sentence'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove a regex pattern\n",
    "\n",
    "import re\n",
    "\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text)\n",
    "    \n",
    "    for i in urls:\n",
    "        input_text = re.sub(i.group().strip(),'',input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = '#[\\w]*'\n",
    "\n",
    "_remove_regex('remove this #hashtag from this sentence', regex_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lexicon_normalization'></a>\n",
    "## 2.2 Lexicon Normalization\n",
    "\n",
    "Another type of textual noise is about the multiple representations exhibited by single word.\n",
    "\n",
    "For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model.\n",
    "\n",
    "The most common lexicon normalization practices are :\n",
    "<a id='lemmatization'></a>\n",
    "\n",
    "   - __Lemmatization__: Lemmatization is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "<a id='stemming'></a>\n",
    "\n",
    "   - __Stemming__:  Stemming, on the other hand, is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "   \n",
    "Below is the sample code that performs lemmatization and stemming using python’s popular library – NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stem = PorterStemmer()\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'multiplying'\n",
    "lem.lemmatize(word,'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipli'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='object_standardization'></a>\n",
    "## 2.3 Object Standardization\n",
    "\n",
    "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
    "\n",
    "Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', 'this', 'is', 'a', 'retweeted', 'awsm', 'dm', 'by', 'Shivam', 'Bansal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted awesome direct message by Shivam Bansal'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'rt':'Retweet','dm':'direct message','awsm':'awesome','luv':'love'}\n",
    "\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split()\n",
    "    print(words)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words)\n",
    "    return new_text\n",
    "    \n",
    "_lookup_words(\"RT this is a retweeted awsm dm by Shivam Bansal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Coreference Resolution'></a>\n",
    "## 2.4 Speech to text\n",
    "\n",
    "Sometimes We'll come across the data Source as Voice , where we need to process them into text first and then start the work on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say something\n",
      "done\n",
      "hi this is a small recording for the first time recording to text audio program part 1 thank you\n"
     ]
    }
   ],
   "source": [
    "r = sr.Recognizer()\n",
    "audio = 'audio.wav'\n",
    "with sr.AudioFile(audio) as source:\n",
    "    print(\"say something\" )\n",
    "    audio = r.record(source)\n",
    "    print(\"done\")\n",
    "try:\n",
    "    text = r.recognize_google(audio)\n",
    "    print(text)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.Text to Features (Feature Engineering on text data)\n",
    "\n",
    "To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques – Syntactical Parsing, Entities / N-grams / word-based features, Statistical features, and word embeddings. Read on to understand these techniques in detail.\n",
    "<a id='Syntactical Parsing'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Part of Speech Tagging'></a>\n",
    "__Part of speech tagging__ – Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. H ere is a list of all possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input text. (it provides several implementations, the default one is perceptron tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('Working', 'VBG'), ('on', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('for', 'IN'), ('last', 'JJ'), ('few', 'JJ'), ('Month', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "text = 'I am Working on Natural Language Processing for last few Month'\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech tagging is used for many important purposes in NLP:\n",
    "\n",
    "__A.Word sense disambiguation:__ Some language words have multiple meanings according to their usage. For example, in the two sentences below:\n",
    "\n",
    "_I. “Please book my flight for Delhi”_\n",
    "\n",
    "_II. “I am going to read this book in the flight”_\n",
    "\n",
    "“Book” is used with different context, however the part of speech tag for both of the cases are different. In sentence I, the word “book” is used as verb, while in II it is used as noun.**Just for the Reference** ([Lesk Algorithm](https://en.wikipedia.org/wiki/Lesk_algorithm) is also us ed for similar purposes)\n",
    "\n",
    "__B.Improving word-based features:__ A learning model could learn different contexts of a word when used word as the features, however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:\n",
    "\n",
    "_Sentence -“book my flight, I will read this book”_\n",
    "\n",
    "_Tokens – (“book”, 2), (“my”, 1), (“flight”, 1), (“I”, 1), (“will”, 1), (“read”, 1), (“this”, 1)_\n",
    "\n",
    "_Tokens with POS _– (“book_VB”, 1), (“my_PRP”, 1), (“flight_NN”, 1), (“I_PRP”, 1), (“will_MD”, 1), (“read_VB”, 1), (“this_DT”, 1), (“book_NN”, 1)\n",
    "\n",
    "__C. Normalization and Lemmatization:__ POS tags are the basis of lemmatization process for converting a word to its base form (lemma).\n",
    "\n",
    "__D.Efficient stopword removal__ : POS tags are also useful in efficient removal of stopwords.\n",
    "\n",
    "For example, there are some tags which always define the low frequency / less important words of a language. For example: (__IN__ – “within”, “upon”, “except”), (**CD** – “one”,”two”, “hundred”), (**MD** – “may”, “must”, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Entity Parsing'></a>\n",
    "\n",
    "## 3.2 Entity Extraction (Entities as features)\n",
    "\n",
    "Entities are defined as the most important chunks of a sentence – noun phrases, verb phrases or both. Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing. The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights.\n",
    "\n",
    "\n",
    "Topic Modelling & Named Entity Recognition are the two key entity detection methods in NLP.\n",
    "\n",
    "<a id='Named Entity Recognition'></a>\n",
    "__A. Named Entity Recognition (NER)__\n",
    "The process of detecting the named entities such as person names, location names, company names etc from the text is called as __NER__. For example :\n",
    "\n",
    "_Sentence – Sergey Brin, the manager of Google Inc. is walking in the streets of New York._\n",
    "\n",
    "_Named Entities –  ( “person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)_\n",
    "\n",
    "A typical NER model consists of <u>three</u> blocks:\n",
    "\n",
    "__Noun phrase identification:__ This step deals with extracting all the noun phrases from a text using dependency parsing and part of speech tagging.\n",
    "\n",
    "__Phrase classification:__ This is the classification step in which all the extracted noun phrases are classified into respective categories (locations, names etc). Google Maps API provides a good path to disambiguate locations, Then, the open databases from dbpedia, wikipedia can be used to identify person names or company names. Apart from this, one can curate the lookup tables and dictionaries by combining information from different sources.\n",
    "\n",
    "__Entity disambiguation__: Sometimes it is possible that entities are misclassified, hence creating a validation layer on top of the results is useful. Use of knowledge graphs can be exploited for this purposes. The popular knowledge graphs are – Google Knowledge Graph, IBM Watson and Wikipedia. \n",
    "\n",
    " \n",
    "<a id='Topic Modeling'></a>\n",
    "\n",
    "## B. Topic Modeling (UnSupervised Learning)\n",
    "\n",
    "Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "__Latent Dirichlet Allocation (LDA)__ is the most popular topic modelling technique, Following is the code to implement topic modeling using LDA in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NullHandler', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_matutils', 'corpora', 'interfaces', 'logger', 'logging', 'matutils', 'models', 'parsing', 'scripts', 'similarities', 'summarization', 'topic_coherence', 'utils']\n",
      "Dictionary(34 unique tokens: ['My', 'Sugar', 'bad', 'but', 'consume.']...)\n",
      "father.\n",
      "0\n",
      "Number of words in dictionary: 34\n",
      "0 My\n",
      "1 Sugar\n",
      "2 bad\n",
      "3 but\n",
      "4 consume.\n",
      "5 father.\n",
      "6 have\n",
      "7 is\n",
      "8 likes\n",
      "9 my\n",
      "10 not\n",
      "11 sister\n",
      "12 sugar,\n",
      "13 to\n",
      "14 a\n",
      "15 around\n",
      "16 dance\n",
      "17 driving\n",
      "18 father\n",
      "19 lot\n",
      "20 of\n",
      "21 practice.\n",
      "22 spends\n",
      "23 time\n",
      "24 Doctors\n",
      "25 and\n",
      "26 blood\n",
      "27 cause\n",
      "28 increased\n",
      "29 may\n",
      "30 pressure.\n",
      "31 stress\n",
      "32 suggest\n",
      "33 that\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "\n",
    "doc_complete = [doc1,doc2,doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "import gensim \n",
    "print(dir(gensim))\n",
    "#from gensim.corpora import Dictionary\n",
    "from gensim import corpora\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = gensim.corpora.Dictionary(doc_clean)\n",
    "\n",
    "print(dictionary)\n",
    "#dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary[5])\n",
    "print(dictionary.token2id['My'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Statistical Features'></a>\n",
    "## 3.3 Statistical Features\n",
    "\n",
    "Text data can also be quantified directly into numbers using several techniques described in this section:\n",
    "\n",
    "<a id='TF – IDF'></a>\n",
    "__A.  Term Frequency – Inverse Document Frequency (TF – IDF)__\n",
    "\n",
    "\n",
    "TF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example – let say there is a dataset of N text documents, In any document “D”, TF and IDF will be defined as –\n",
    "\n",
    "Term Frequency (TF) – TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
    "\n",
    "Inverse Document Frequency (IDF) – IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T.\n",
    "\n",
    "TF . IDF – TF IDF formula gives the relative importance of a term in a corpus (list of documents), given by the following formula below. Following is the code using python’s scikit learn package to convert a text into tf idf vectors:\n",
    "\n",
    "**In Short :-**\n",
    "\n",
    "**Term Frequency:** This summarizes how often a given word appears within a document.<br>\n",
    "**Inverse Document Frequency:** This downscales words that appear a lot across documents\n",
    "\n",
    "\n",
    "<img src='./Images/image-4.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):              # TF is Term frequency (\"This summarizes how often a given word appears within a document.\")\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):           # IDF is Term frequency (\"This summarizes how often a given word appears within a document.\")\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = tb(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "\n",
    "document2 = tb(\"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\")\n",
    "\n",
    "document3 = tb(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: python, TF-IDF: 0.01662\n",
      "\tWord: films, TF-IDF: 0.00997\n",
      "\tWord: made-for-TV, TF-IDF: 0.00665\n",
      "Top words in document 2\n",
      "\tWord: genus, TF-IDF: 0.02192\n",
      "\tWord: 2, TF-IDF: 0.02192\n",
      "\tWord: from, TF-IDF: 0.01096\n",
      "Top words in document 3\n",
      "\tWord: Colt, TF-IDF: 0.01367\n",
      "\tWord: Magnum, TF-IDF: 0.01367\n",
      "\tWord: revolver, TF-IDF: 0.01367\n"
     ]
    }
   ],
   "source": [
    "bloblist = [document1, document2, document3]\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical word frequencies (With Matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The contamination or spoliation of the natural environment is known as pollution. It disturbs the natural process of the environment. Environmental pollution also causes harm to our environment by disturbing the natural balance. There are different types of environmental pollution such as air pollution, water pollution, land pollution, noise pollution etc.There are different causes of environmental pollution. Among them, waste materials of different industries, ejaculation of poisonous gases, deforestation, smoke emitted by vehicles or factories are the major factors that cause environmental pollution. In modern world environmental pollution has become a serious issue for the whole world. Due to environmental pollution, the temperature of the earth is increasing day by day. The air of the earth is no longer remaining fresh and sweet. People are suffering from many diseases in every corner of the world. Again in the metropolitan cities increasing numbers of vehicles not only causes air pollution but also disturbs our ears by causing noise pollution. In this century everyone is racing for industrialization or development. But this type of blind races may destroy the greenery in our environment.On the other hand water pollution is another type of environmental pollution. In our country in most of the areas river water is the only source of drinking water. But almost every river in India is in the grip of pollution due to the negligence of people. Poisonous waste materials from industries are thrown in the rivers and as a result of that, the river water gets polluted. People also pollute river water in the name of traditional beliefs. For example, people still believe that the ashes (Asthi) after burial ceremonies should be thrown in the river, the hair needs to be thrown in the river after Mundan etc. Water pollution gives birth to different water born disease.Environmental pollution needs to be stopped to secure the earth for our successors. We should keep our planet healthy to keep ourselves fit and healthy.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The contamination or spoliation of the natural environment is known as pollution. It disturbs the natural process of the environment. Environmental pollution also causes harm to our environment by disturbing the natural balance. There are different types of environmental pollution such as air pollution, water pollution, land pollution, noise pollution etc.There are different causes of environmental pollution. Among them, waste materials of different industries, ejaculation of poisonous gases, deforestation, smoke emitted by vehicles or factories are the major factors that cause environmental pollution. In modern world environmental pollution has become a serious issue for the whole world. Due to environmental pollution, the temperature of the earth is increasing day by day. The air of the earth is no longer remaining fresh and sweet. People are suffering from many diseases in every corner of the world. Again in the metropolitan cities increasing numbers of vehicles not only causes air pollution but also disturbs our ears by causing noise pollution. In this century everyone is racing for industrialization or development. But this type of blind races may destroy the greenery in our environment.On the other hand water pollution is another type of environmental pollution. In our country in most of the areas river water is the only source of drinking water. But almost every river in India is in the grip of pollution due to the negligence of people. Poisonous waste materials from industries are thrown in the rivers and as a result of that, the river water gets polluted. People also pollute river water in the name of traditional beliefs. For example, people still believe that the ashes (Asthi) after burial ceremonies should be thrown in the river, the hair needs to be thrown in the river after Mundan etc. Water pollution gives birth to different water born disease.Environmental pollution needs to be stopped to secure the earth for our successors. We should keep our planet healthy to keep ourselves fit and healthy.\"\n",
    "text          # Row txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_T = text.split() #  splitted into words \n",
    "split_T1 = []        # Creating blank list to pass on the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_wo = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_wo = list(stopwords.words('english')) # importing stopwords to get rid of the extra words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in split_T:\n",
    "    if i not in stop_wo:     # appending those words which aren't useful\n",
    "        split_T1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(split_T1) # plotting on the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq.plot(8, cumulative=False,c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Important tasks of NLP'></a>\n",
    "## 4. Important tasks of NLP\n",
    "\n",
    "This section talks about different use cases and problems in the field of natural language processing.\n",
    "\n",
    "<a id='Text Classification'></a>\n",
    "__4.1 Text Classification__\n",
    "\n",
    "Text classification is one of the classical problem of NLP. Notorious examples include – _Email Spam Identification, topic classification of news, sentiment classification and organization of web pages by search engines_.\n",
    "\n",
    "Text classification, in common words is defined as a technique to systematically classify a text object (document or sentence) in one of the fixed category. It is really helpful when the amount of data is too large, especially for organizing, information filtering, and storage purposes.\n",
    "\n",
    "A typical natural language classifier consists of two parts: (a) Training (b) Prediction as shown in image below. Firstly the text input is processes and features are created. The machine learning models then learn these features and is used for predicting against the new text.\n",
    "\n",
    "<img src='./Images/image-5.png'>\n",
    "\n",
    "Here is a code that uses naive bayes classifier using text blob library (built on top of nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('I hate baseball.', 'Class_B'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   (\"cricket is awesome\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), \n",
    "    ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "model = NBC(training_corpus) \n",
    "print(model.classify(\"cricket is a game.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahmead = [('I am exhausted of this work.', 'Class_B'), \n",
    "           ('This is my best work.', 'Class_A')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n"
     ]
    }
   ],
   "source": [
    "print(model.classify(\"cricket is eleven players game\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.33333333333334 %\n"
     ]
    }
   ],
   "source": [
    "print(model.accuracy(test_corpus) * 100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Coreference Resolution'></a>\n",
    "## 4.3 Coreference Resolution\n",
    "\n",
    "Coreference Resolution is a process of finding relational links among the words (or phrases) within the sentences. Consider an example sentence: ” Donald went to John’s office to see the new table. He looked at it for an hour.“\n",
    "\n",
    "Humans can quickly figure out that “he” denotes Donald (and not John), and that “it” denotes the table (and not John’s office). Coreference Resolution is the component of NLP that does this job automatically. It is used in document summarization, question answering, and information extraction. Stanford CoreNLP provides a python wrapper for commercial purposes.\n",
    "\n",
    " \n",
    "<a id='Other'></a>\n",
    "## 4.4 Other NLP problems / tasks\n",
    "\n",
    "- Text Summarization – Given a text article or paragraph, summarize it automatically to produce most important and relevant sentences in order.\n",
    "\n",
    "- Machine Translation – Automatically translate text from one human language to another by taking care of grammar, semantics and information about the real world, etc.\n",
    "\n",
    "- Natural Language Generation and Understanding – Convert information from computer databases or semantic intents into readable human language is called language generation. Converting chunks of text into more logical structures that are easier for computer programs to manipulate is called language understanding.\n",
    "\n",
    "- Optical Character Recognition – Given an image representing printed text, determine the corresponding text.\n",
    "\n",
    "- Document to Information – This involves parsing of textual data present in documents (websites, files, pdfs and images) to analyzable and clean format.\n",
    " \n",
    "<a id='Important Libraries'></a>\n",
    "## 5. Important Libraries for NLP (python)\n",
    "\n",
    "- Scikit-learn: Machine learning in Python\n",
    "- Natural Language Toolkit (NLTK): The complete toolkit for all NLP techniques.\n",
    "- Pattern – A web mining module for the with tools for NLP and machine learning.\n",
    "- TextBlob – Easy to use nl p tools API, built on top of NLTK and Pattern.\n",
    "- spaCy – Industrial strength N LP with Python and Cython.\n",
    "- Gensim – Topic Modelling for Humans\n",
    "- Stanford Core NLP – NLP services and packages by Stanford NLP Group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
